\documentclass[main.tex]{subfiles}
\begin{document}
\subsection{Day 4: 8/29/22}

\subsubsection{Subspaces and Linear (In)dependence}

\begin{definition}[Subspace]
    Suppose we have some subset $V\subseteq \RR^n$. We call $V$ a \vocab{subspace} of $\RR^n$ if:
    \begin{enumerate}
        \item $V$ contains the zero vector $\vec{0}$.
        \item $V$ is closed under addition and scalar multiplication, i.e. whenever we have vectors $\vec{x}, \vec{y}\in V$ and two scalars $\lambda , \mu\in \RR$, we must have $\lambda \vec{x} + \mu \vec{y} \in V$.
    \end{enumerate}
\end{definition}
Notably, the empty set is not a subspace due to the first condition.

\begin{example}
    Here are some examples of subspaces of $\RR^n$:
    \begin{enumerate}
        \item $V = \{\vec{0}\}$ (trivial subspace).
        \item $V = \RR^n$.
    \end{enumerate}
\end{example}

These are trivial to check for both conditions. We now present a nontrivial subspace by way of definition:

\begin{definition}[Span]
    Suppose $\vec{v}_1, \ldots , \vec{v}_m\in \RR^n$. We define the \vocab{span} of $\vec{v}_1 \ldots , \vec{v}_m$ as the set
    \[\{c_1\vec{v}_1 + \ldots + c_m\vec{v}_m : c_1, \ldots , c_m \in \RR\},\]
    i.e. the set of linear combinations of the $m$ vectors $\vec{v}_1, \ldots , \vec{v}_m$.
\end{definition}

Here are some examples of spans:
\begin{example}
    In $\RR^n$,
    \begin{align*}
        \vspan\{\vec{e}_1\} &= \{c\vec{e}_1 : c\in \RR\} \\
        &= \{c(1, 0, \ldots , 0) : c\in \RR\} \\
        &= \{(c, 0, \ldots , 0) : c \in \RR\},
    \end{align*}
    or heuristically the $x_1$-axis in $\RR^n$.
\end{example}

\begin{example}
    In $\RR^n$,
    \begin{align*}
        \vspan\{\vec{e}_1, \ldots, \vec{e}_n\} &= \{c_1\vec{e}_1 + \ldots + c_n\vec{e}_n : c_1, \ldots , c_n\in \RR\} \\
        &= \{c_1(1, 0,\ldots , 0) + \ldots + c_n(0, 0, \ldots , 0, 1) : c_1, \ldots , c_n \in \RR\} \\
        &= \{(c_1, \ldots , c_n) : c_1, \ldots , c_n \in \RR\} \\
        &= \RR^n.
    \end{align*}
\end{example}

Now let's prove spans are subspaces:
\begin{lemma}
    Suppose $\vec{v}_1, \ldots , \vec{v}_n\in \RR^n$. Then $\vspan\{\vec{v}_1, \ldots , \vec{v}_m\}\in \RR^n$ is a subspace of $\RR^n$.
\end{lemma}

\begin{proof}
    We need to check the two subspace conditions. First, to check that $\vec{0}$ is in our span, we can simply pick $0 = c_1 = c_2 = \ldots = c_m$ to get a linear combination of our vectors that is equal to $\vec{0}$. Suppose we have arbitrary $\vec{x}, \vec{y}\in \vspan\{\vec{v}_1, \ldots , \vec{v}_m\}$ and $\lambda, \mu \in \RR$. Since $\vec{x}, \vec{y}\in \vspan\{\vec{v}_1 \ldots , \vec{v}_m\}$, it follows that $\vec{x} = c_1\vec{v}_1 + \ldots + c_m\vec{v}_m$ for $c_1, \ldots , c_m\in \RR$ and $\vec{y} = k_1\vec{v}_1 + \ldots + k_m\vec{v}_m$ for $k_1, \ldots , k_m\in \RR$. Then
    \begin{align*}
        \lambda \vec{x} + \mu \vec{y} &= \lambda (c_1\vec{v}_1 + \ldots + c_m\vec{v}_m) + \mu (k_1\vec{v}_1 + \ldots + k_m\vec{v}_m)\\
        &= (\lambda c_1 + \mu k_1) \vec{v}_1 + \ldots + (\lambda c_m + \mu k_m)\vec{v}_m
    \end{align*}
    which is a linear combination of the vectors $\vec{v}_1 , \ldots , \vec{v}_m$, so this new vector must also be in the span and the second condition is true.
\end{proof}

\begin{definition}[Linear (In)dependence]
    A collection of vectors $\vec{v}_1, \ldots , \vec{v}_m\in \RR^n$ is called \vocab{linearly dependent} if there is a linear combination $c_1\vec{v}_1 + \ldots + c_m\vec{v}_m = \vec{0}$ for some $c_1, \ldots , c_m\in \RR$ such that the vector $(c_1, \ldots , c_m) \neq \vec{0}$. Note that this $\vec{0}\in \RR^m\neq \RR^n$. The collection of vectors is called \vocab{linearly independent} otherwise, i.e. if $c_1, \ldots , c_m\in \RR$ and
    \[c_1\vec{v}_1 + \ldots + c_m\vec{v}_m = \vec{0}\implies (c_1, \ldots , c_m) = \vec{0}.\]
\end{definition}

Here's some examples of linearly (in)dependent vectors:
\begin{example}
    A collection of a single $\vec{v}\in \RR^n$ is linearly dependent if and only if $\vec{v} = \vec{0}$.
\end{example}

\begin{proof}
    The $\impliedby$ direction is trivial (just pick any nonzero $c$), so suppose that $\{\vec{v}\}$ is linearly dependent. Then we must have $c\vec{v} = \vec{0}$ with $c\neq 0$. Since $c\neq 0$, it follows that $\vec{v} = \frac{1}{c}\vec{0} = \vec{0}$.
\end{proof}

\begin{example}
    A collection of two vectors $\vec{v}, \vec{w}\in \RR$ is linearly dependent if and only if $\vec{v} = \lambda \vec{w}$ or $\vec{w} = \lambda\vec{v}$ for some $\lambda \in \RR$.
\end{example}

\begin{proof}
    $(\implies)$: Assume $\vec{v}, \vec{w}$ are linearly dependent, i.e. we must have $c_1\vec{v} + c_2\vec{w} = \vec{0}$ for some $c_1, c_2\in \RR$ with $(c_1, c_2)\neq \vec{0}$. There's two cases. Suppose $c_1\neq 0$; then we have $\vec{v} = -\frac{c_2}{c_1}\vec{w}$, with $\lambda = -\frac{c_2}{c_1}$. Otherwise suppose $c_2\neq 0$; then we have $\vec{w} = -\frac{c_1}{c_2}\vec{v}$, with $\lambda = -\frac{c_1}{c_2}$. Either way one has to be a multiple of the other.

    $(\impliedby)$: There's two cases. First assume $\vec{v} = \lambda\vec{w}$ for some $\lambda\in \RR$. Then $1\vec{v} + (-\lambda)\vec{w} = \vec{0}$, so $\vec{v}, \vec{w}$ are linearly dependent. Next, assume $\vec{w} = \lambda\vec{v}$ for some $\lambda\in \RR$. Then $(-\lambda)\vec{v} + 1\vec{w} = \vec{0}$, so $\vec{v}, \vec{w}$ are linearly dependent.
\end{proof}

We will prove the general theorem tomorrow.
\end{document}