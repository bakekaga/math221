\documentclass[main.tex]{subfiles}
\begin{document}
\subsection{Day 17: 9/30/22}
Recall from the last lecture that we found how to get $\rref A = (r_{ij})$ from $A$ using Gaussian Elimination, and $N(A) = N(\rref A)$. Also recall that if $\rref A$ had pivot columns $j_1, \ldots , j_Q$, then $N(A)$ had a basis 
\[\{\vec{e}_j - \sum_{\ell = 1}^Qr_{\ell j}\vec{e}_{j\ell}: j\in \{1, \ldots , n\}\setminus \{j_1, \ldots , j_Q\}\}\implies \dim N(A) = n - Q.\]

\begin{proposition}
    Let $A = (\vec{\alpha}_1, \ldots , \vec{\alpha}_n)$ be a $m\times n$ matrix with pivot columns $j_1, \ldots , j_Q$. Then $\vec{\alpha}_{j_1}, \ldots , \vec{\alpha}_{j_Q}$ form a basis for $C(A)$.
\end{proposition}

\begin{proof}
    Note that $\dim C(A) = n - \dim N(A) = (n - (n - Q)) = Q$ by Rank-Nullity. So to prove the $Q$ vectors $\vec{\alpha}_{j_1}, \ldots , \vec{\alpha}_{j_Q}$ form a basis, it suffices to show that they span $C(A)$, i.e. $\vspan\{\vec{\alpha}_{j_1}, \ldots , \vec{\alpha}_{j_Q}\} = \vspan\{\vec{\alpha}_{1}, \ldots , \vec{\alpha}_{n}\}$ because of Problem Set 2 Problem 3. Observe that trivially $\vspan\{\vec{\alpha}_{j_1}, \ldots , \vec{\alpha}_{j_Q}\} \subseteq \vspan\{\vec{\alpha}_{1}, \ldots , \vec{\alpha}_{m}\}$, so we just need to show that $ \vspan\{\vec{\alpha}_{1}, \ldots , \vec{\alpha}_{n}\}\subseteq \vspan\{\vec{\alpha}_{j_1}, \ldots , \vec{\alpha}_{j_Q}\}$. Consider any $j \in \{1, \ldots , n\}$. If $j\in \{j_1, \ldots , j_Q\}$, then obviously $\vec{\alpha}_j\in \vspan\{\vec{\alpha}_{j_1}, \ldots , \vec{\alpha}_{j_Q}\}$. On the other hand, if $j \in \{1, \ldots , n\}\setminus \{j_1, \ldots , j_Q\}$, we know that 
    \begin{align*}
        &\vec{e}_j - \sum_{\ell = 1}^Qr_{\ell j}\vec{e}_{j\ell}\in N(A) \\
        &\iff A\left(\vec{e}_j - \sum_{\ell = 1}^Qr_{\ell j}\vec{e}_{j\ell}\right) = \vec{0} \\
        &\iff \vec{\alpha}_j - \sum_{\ell = 1}^\alpha r_{\ell j}\vec{\alpha}_{j\ell} = \vec{0} \\
        &\iff \vec{\alpha}_j = \sum_{\ell = 1}^Q r_{\ell j}\vec{\alpha}_{j_\ell}\in \vspan\{\vec{\alpha}_{j_1}, \ldots , \vec{\alpha}_{j_Q}\}
    \end{align*}
    so $\vspan\{\vec{\alpha}_{1}, \ldots , \vec{\alpha}_{n}\}\subseteq \vspan\{\vec{\alpha}_{j_1}, \ldots , \vec{\alpha}_{j_Q}\}$ as desired.
\end{proof}

\subsubsection{Inhomogeneous Systems}

\begin{example}
Solve
\[\left\{\begin{aligned}
&0.5x_1 + x_2 + x_3 = 1 \\
&2x_1 + 5x_2 + x_3 = 2
\end{aligned}\right.\]
\end{example}
Expressing the system as an augmented matrix, we have
\[\left(\begin{array}{@{}c c c | c@{}}
    0.5 & 1 & 1 & 1 \\
    2 & 5 & 1 & 2
\end{array}\right)\overset{\text{row reduction}}{\iff} \left(\begin{array}{@{}c c c | c@{}}
    1 & 0 & 8 & 6 \\
    0 & 1 & -3 & -2
\end{array}\right).\]
We can note that $j_1 = 1, j_2 = 2$, and since there are no coefficient rows with all zeroes followed by a nonzero element, this is a consistent system with solutions satisfying
\[\vec{x} = \begin{pmatrix} x_1 \\ x_2 \\ x_2 \end{pmatrix} \in \left\{\begin{pmatrix} 6 - 8x_3 \\ -2 - 3x_3 \\ x_3 \end{pmatrix} : x_3 \in \RR\right\} = \left\{\begin{pmatrix} 6 \\ -2 \\ 0\end{pmatrix} + x_3\begin{pmatrix} -8 \\ -3 \\ 1 \end{pmatrix} : x_3 \in \RR\right\}.\]
Note that $x_3$ is just by itself because it's not a pivot variable, and that $(-8, -3, 1)^\top$ is a basis vector for the null space of the coefficient matrix $A$. Thus, we can finally rewrite this as
\[\begin{pmatrix} 6 \\ -2 \\ 0\end{pmatrix} + \vspan\left\{\begin{pmatrix} -8 \\ -3 \\ 1 \end{pmatrix}\right\} = \begin{pmatrix} 6 \\ -2 \\ 0\end{pmatrix} + N(A).\]
In particular, this is the null space of the coefficient matrix (the set of solutions to $A\vec{x} = \vec{0}$ translated by the vector $(6, -2, 0)^\top$ to get the set of solutions to $A\vec{x} = (1, 2)^\top$. Here is the general form:

\begin{lemma}
    Consider an inhomogeneous system of $m$ equations and $n$ unknowns, or equivalently $A\vec{x} = \vec{b}$, where $A$ is an $m\times n$ matrix and $\vec{x}\in \RR^n, \vec{b}\in \RR^m$. Then:
    \begin{enumerate}
        \item The system is consistent (i.e. $A\vec{x} = \vec{b}$ has at least one solution) if and only if $\vec{b}\in C(A)$.
        \item When this system is consistent and $\vec{x}_0\in \RR^n$ is some (``particular") solution, then the set of all solutions is $\vec{x}_0 + N(A)$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    This is actually not bad to prove.
    \begin{enumerate}
        \item Observe that this system is consistent if and only if $\vec{b} = A\vec{x}$ for at least one $\vec{x}\in \RR^n$ if and only if $\vec{b} \in \{A\vec{x} : \vec{x}\in \RR^n\} = C(A)$.
        \item Let $\vec{x}\in \RR^n$ be an arbitrary solution. Then $A\vec{x} = \vec{b} = A\vec{x}_0 \implies A(\vec{x} - \vec{x}_0) = \vec{0}\implies \vec{x} - \vec{x}_0 \in N(A)$, i.e. $\vec{x}\in \vec{x}_0 + N(A)$. On the other hand, if $\vec{x}\in \vec{x}_0 + N(A)\implies \vec{x} - \vec{x}_0 \in N(A)$, then 
        \[A\vec{x} = A(\vec{x} - \vec{x}_0 + \vec{x}_0) = A(\vec{x} - \vec{x}_0) + A\vec{x}_0 = \vec{0} + \vec{b} = \vec{b},\]
        so $A\vec{x} = \vec{b}\iff \vec{x}\in \vec{x}_0 + N(A)$ as desired.\qedhere
    \end{enumerate}
\end{proof}

Now that we're done with Chapter 1, let's provide some motivation for Chapter 2.

Observe that in $\RR$, we defined $\limn x_n = y$ if and only if eventually all $x_n$ lied within an open interval $(y - \varepsilon, y + \varepsilon)$, or in other words for all $\varepsilon > 0$, there exists $N\in \NN$ such that $\abs{x_n - y} < \varepsilon$ for all $n\ge N$.

To extend this to $\RR^n$, we define $\limn \vec{x}^{(n)} = \vec{y}$ if and only if eventually all $\vec{x}^{(n)}$ lies within an open ball $B_\varepsilon (y) = \{\vec{x} \in \RR^n : \norm{\vec{x} - \vec{y}} < \varepsilon\}$, or in other words for all $\varepsilon > 0$, there exists $N\in \NN$ such that $\norm{\vec{x}^{(n)} - \vec{y}} < \varepsilon$ for all $k\ge N$ if and only if $\limn \norm{\vec{x}^{(n)} - \vec{y}} = 0$. Note that this is a sequence of real numbers!
\end{document}